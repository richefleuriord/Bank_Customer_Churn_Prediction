{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b0d340-fa4a-4001-a858-41e406f3c440",
   "metadata": {},
   "source": [
    "  # **AKADEMI EDUCATION**\n",
    "# **Première cohorte (2025): Science des données et intelligence artificielle**\n",
    "#### *Phase 5: PROJET DE SCIENCE DES DONNÉES*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a1f0e-0e5f-4c2b-8c1c-00660bb3d88c",
   "metadata": {},
   "source": [
    "**Noms des étudiants du groupe: Riché FLEURINORD et Micka LOUIS**   \n",
    "**Rythme d’apprentissage: Autonome**  \n",
    "**Date de soutenance: 27 octobre 2025**  \n",
    "**Noms des instructeurs: Wedter JEROME et Geovany Batista Polo LAGUERRE**  \n",
    "**Lien de l’article de blog (lien du dépôt GitHub): https://github.com/richefleuriord/Bank_Customer_Churn_Prediction.git**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9508467-8365-49cd-b508-53c98ce472d6",
   "metadata": {},
   "source": [
    "# *3-Préparation des données*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf1445-d417-40b3-9493-79b892fcd2d4",
   "metadata": {},
   "source": [
    "## Introduction et Objectifs de la Préparation des Données\n",
    "\n",
    "La **préparation des données** constitue une phase fondamentale dans tout projet de *Data Science*, car la qualité du modèle dépend directement de la qualité du jeu de données.  \n",
    "Elle vise à transformer les données brutes issues du système bancaire en un format propre, cohérent et exploitable pour la modélisation prédictive.\n",
    "\n",
    "Dans ce projet, l’objectif est de **préparer le jeu de données des clients bancaires** afin de construire plusieurs modèles capables de prédire le **risque de départ (churn)** des clients, en optimisant la performance et la robustesse de chaque algorithme.\n",
    "\n",
    "---\n",
    "\n",
    "### Objectifs spécifiques\n",
    "\n",
    "- Nettoyer et vérifier la cohérence du jeu de données.  \n",
    "- Créer de nouvelles variables pertinentes à partir des données existantes (*feature engineering*).  \n",
    "- Transformer les variables catégorielles en variables numériques exploitables par les algorithmes de *Machine Learning*.  \n",
    "- Mettre en place un prétraitement **adapté à chaque famille de modèles** :  \n",
    "  - Normalisation et équilibrage (SMOTE) pour les modèles sensibles aux échelles et au déséquilibre (*ex. Régression Logistique*).  \n",
    "  - Paramètres internes de pondération des classes pour les modèles robustes (*ex. Random Forest, XGBoost*).  \n",
    "- Séparer les données en trois ensembles :  \n",
    "  - **Train (70 %)** : pour l’apprentissage du modèle.  \n",
    "  - **Validation (15 %)** : pour l’ajustement des hyperparamètres.  \n",
    "  - **Test (15 %)** : pour l’évaluation finale.  \n",
    "- Sauvegarder les objets essentiels (*scaler, colonnes finales, modèles entraînés*) pour leur réutilisation dans l’application Streamlit.\n",
    "\n",
    "---\n",
    "\n",
    "### Étapes principales du pipeline de préparation\n",
    "\n",
    "1. **Vérifications de base** : détection des valeurs manquantes, doublons et inspection de la distribution de la variable cible.  \n",
    "2. **Feature Engineering** : création de nouvelles variables (ratios, regroupement d’âges, interaction entre indicateurs d’activité, etc.) et suppression des variables inutiles.  \n",
    "3. **Encodage des variables catégorielles** avec le *One-Hot Encoding*.  \n",
    "4. **Séparation du jeu de données** en ensembles d’entraînement, de validation et de test tout en conservant la distribution de la cible.  \n",
    "5. **Normalisation sélective** : utilisation du `StandardScaler` uniquement pour les modèles linéaires (Régression Logistique).  \n",
    "6. **Gestion du déséquilibre** :  \n",
    "   - Application de **SMOTE** dans le pipeline de la Régression Logistique.  \n",
    "   - Utilisation de `class_weight='balanced'` pour le **Random Forest**.  \n",
    "   - Ajustement de `scale_pos_weight` pour le **XGBoost**.  \n",
    "7. **Sauvegarde des objets et jeux de données préparés** pour la phase de modélisation et le déploiement Streamlit.\n",
    "\n",
    "---\n",
    "\n",
    "*Cette approche garantit une préparation des données rigoureuse, flexible et adaptée à chaque modèle, assurant des prédictions plus fiables et une performance optimale du système de détection du churn.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aad55167-a96d-4dd3-be6b-f4a72d0ad962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement réussi\n",
      "Nombre de lignes : 10000\n",
      "Nombre de colonnes : 18\n",
      "\n",
      "--- Vérifications de base ---\n",
      "Valeurs manquantes totales : 0\n",
      "Doublons : 0\n",
      "\n",
      "Répartition de la variable cible (Exited) :\n",
      "Exited\n",
      "0    79.62\n",
      "1    20.38\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "--- Feature Engineering ---\n",
      "\n",
      " Étape de préparation commune terminée.\n",
      "\n",
      "--- Construction des pipelines par modèle ---\n",
      " Pipeline Régression Logistique entraîné et sauvegardé.\n",
      " Modèle Random Forest entraîné et sauvegardé.\n",
      "✔ Modèle XGBoost entraîné et sauvegardé.\n",
      "\n",
      " Données sauvegardées avec succès.\n",
      " Pipeline complet terminé avec succès.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PRÉPARATION DES DONNÉES\n",
    "# ============================================================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ============================================================\n",
    "# CHARGEMENT DU JEU DE DONNÉES\n",
    "# ============================================================\n",
    "df = pd.read_csv(\"../Data/Customer-Churn-Records.csv\")\n",
    "\n",
    "print(\"Chargement réussi\")\n",
    "print(\"Nombre de lignes :\", df.shape[0])\n",
    "print(\"Nombre de colonnes :\", df.shape[1])\n",
    "\n",
    "# ============================================================\n",
    "# VÉRIFICATIONS DE BASE\n",
    "# ============================================================\n",
    "print(\"\\n--- Vérifications de base ---\")\n",
    "print(\"Valeurs manquantes totales :\", df.isnull().sum().sum())\n",
    "print(\"Doublons :\", df.duplicated().sum())\n",
    "print(\"\\nRépartition de la variable cible (Exited) :\")\n",
    "print(df['Exited'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE ENGINEERING COMMUN\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n--- Feature Engineering ---\")\n",
    "\n",
    "# Suppression des colonnes non pertinentes\n",
    "df = df.drop(columns=[\"CustomerId\", \"RowNumber\", \"Surname\"], errors='ignore')\n",
    "\n",
    "# Suppression de variables inutiles ou fuyantes\n",
    "df = df.drop(columns=[\"Complain\"], errors='ignore')\n",
    "\n",
    "# Ratio Balance / Salaire\n",
    "df['BalanceToSalaryRatio'] = df['Balance'] / (df['EstimatedSalary'] + 1)\n",
    "\n",
    "# Regroupement des âges : Jeune, Moyen, Âgé\n",
    "df['AgeCategory'] = pd.cut(\n",
    "    df['Age'],\n",
    "    bins=[17, 35, 55, 100],\n",
    "    labels=['Jeune', 'Moyen', 'Âgé']\n",
    ")\n",
    "df = df.drop(columns=['Age'], errors='ignore')\n",
    "\n",
    "# Regroupement du nombre de produits\n",
    "df['ProductCategory'] = df['NumOfProducts'].apply(\n",
    "    lambda x: str(x) if x in [1, 2] else '3+'\n",
    ")\n",
    "df = df.drop(columns=['NumOfProducts'], errors='ignore')\n",
    "\n",
    "# Autres variables dérivées\n",
    "df['ActiveCredit'] = df['IsActiveMember'] * df['HasCrCard']\n",
    "df['LowSatisfaction'] = (df['Satisfaction Score'] <= 2).astype(int)\n",
    "\n",
    "# ============================================================\n",
    "# ENCODAGE DES VARIABLES CATÉGORIELLES\n",
    "# ============================================================\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# ============================================================\n",
    "# SÉPARATION FEATURES / CIBLE\n",
    "# ============================================================\n",
    "X = df.drop('Exited', axis=1)\n",
    "y = df['Exited']\n",
    "\n",
    "# ============================================================\n",
    "# SPLIT TRAIN / VAL / TEST\n",
    "# ============================================================\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42\n",
    ")\n",
    "\n",
    "# Alignement des colonnes\n",
    "X_val = X_val.reindex(columns=X_train.columns, fill_value=0)\n",
    "X_test = X_test.reindex(columns=X_train.columns, fill_value=0)\n",
    "\n",
    "# Sauvegarde des colonnes finales\n",
    "columns_final = list(X_train.columns)\n",
    "joblib.dump(columns_final, r\"C:\\Users\\HP\\course\\phase_5\\Models\\columns_final.pkl\")\n",
    "\n",
    "print(\"\\n Étape de préparation commune terminée.\")\n",
    "\n",
    "# ============================================================\n",
    "# PIPELINES SPÉCIFIQUES AUX MODÈLES\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n--- Construction des pipelines par modèle ---\")\n",
    "\n",
    "# ============================================================\n",
    "# PIPELINE - RÉGRESSION LOGISTIQUE (scaling + SMOTE)\n",
    "# ============================================================\n",
    "\n",
    "num_features = [\n",
    "    'CreditScore', 'Tenure', 'Balance',\n",
    "    'EstimatedSalary', 'BalanceToSalaryRatio',\n",
    "    'Satisfaction Score', 'Point Earned'\n",
    "]\n",
    "num_features = [col for col in num_features if col in X_train.columns]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "joblib.dump(scaler, r\"C:\\Users\\HP\\course\\phase_5\\Models\\scaler.pkl\")\n",
    "\n",
    "pipeline_logreg = ImbPipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('smote', SMOTE(random_state=42)),\n",
    "    ('model', LogisticRegression(max_iter=1000, solver='lbfgs'))\n",
    "])\n",
    "\n",
    "pipeline_logreg.fit(X_train, y_train)\n",
    "joblib.dump(pipeline_logreg, r\"C:\\Users\\HP\\course\\phase_5\\Models\\logistic_regression_pipeline.pkl\")\n",
    "\n",
    "print(\" Pipeline Régression Logistique entraîné et sauvegardé.\")\n",
    "\n",
    "# ============================================================\n",
    "# MODÈLE - RANDOM FOREST (pas de scaling, pas de SMOTE)\n",
    "# ============================================================\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    random_state=42,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "joblib.dump(rf, r\"C:\\Users\\HP\\course\\phase_5\\Models\\random_forest_model.pkl\")\n",
    "\n",
    "print(\" Modèle Random Forest entraîné et sauvegardé.\")\n",
    "\n",
    "# ============================================================\n",
    "# MODÈLE - XGBOOST (pas de scaling, pas de SMOTE)\n",
    "# ============================================================\n",
    "\n",
    "neg, pos = np.bincount(y_train)\n",
    "scale_pos_weight = neg / pos\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=400,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    random_state=42,\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    eval_metric='logloss',\n",
    "    n_jobs=-1\n",
    ")\n",
    "xgb.fit(X_train, y_train)\n",
    "joblib.dump(xgb, r\"C:\\Users\\HP\\course\\phase_5\\Models\\xgboost_model.pkl\")\n",
    "\n",
    "print(\"✔ Modèle XGBoost entraîné et sauvegardé.\")\n",
    "\n",
    "# ============================================================\n",
    "# SAUVEGARDE DES DONNÉES POUR VALIDATION ET TEST\n",
    "# ============================================================\n",
    "X_train.to_csv(\"../Data/X_train_prepared.csv\", index=False)\n",
    "y_train.to_csv(\"../Data/y_train_prepared.csv\", index=False)\n",
    "X_val.to_csv(\"../Data/X_val_prepared.csv\", index=False)\n",
    "y_val.to_csv(\"../Data/y_val_prepared.csv\", index=False)\n",
    "X_test.to_csv(\"../Data/X_test_prepared.csv\", index=False)\n",
    "y_test.to_csv(\"../Data/y_test_prepared.csv\", index=False)\n",
    "\n",
    "print(\"\\n Données sauvegardées avec succès.\")\n",
    "print(\" Pipeline complet terminé avec succès.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
