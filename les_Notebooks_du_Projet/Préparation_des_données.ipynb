{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1b0d340-fa4a-4001-a858-41e406f3c440",
   "metadata": {},
   "source": [
    "  # **AKADEMI EDUCATION**\n",
    "# **Première cohorte (2025): Science des données et intelligence artificielle**\n",
    "#### *Phase 5: PROJET DE SCIENCE DES DONNÉES*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a1f0e-0e5f-4c2b-8c1c-00660bb3d88c",
   "metadata": {},
   "source": [
    "**Noms des étudiants du groupe: Riché FLEURINORD et Micka LOUIS**   \n",
    "**Rythme d’apprentissage: Autonome**  \n",
    "**Date de soutenance: 27 octobre 2025**  \n",
    "**Noms des instructeurs: Wedter JEROME et Geovany Batista Polo LAGUERRE**  \n",
    "**Lien de l’article de blog (lien du dépôt GitHub): https://github.com/richefleuriord/Bank_Customer_Churn_Prediction.git**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9508467-8365-49cd-b508-53c98ce472d6",
   "metadata": {},
   "source": [
    "# *3-Préparation des données*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b82dedd-b31c-4f8f-85c6-08d5e9e260af",
   "metadata": {},
   "source": [
    "## Création d’un pipeline de préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff4877-72e8-49d1-b1dc-df0c3fa4afad",
   "metadata": {},
   "source": [
    "Ce notebook prépare le dataset pour l'entraînement des modèles de prédiction du churn.  \n",
    "Les étapes incluent :\n",
    "1. Suppression des colonnes inutile\n",
    "2. Encodage des variables catégorielles\n",
    "3. Standardisation des variables numériques\n",
    "4. Séparation train / validation / test\n",
    "5. Pipeline complet pour faciliter la modélisation et la validation croisée\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dddb766c-9d8c-4172-b6b9-d8076d236c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_prepared shape : (6000, 13)\n",
      "X_val_prepared shape : (2000, 13)\n",
      "X_test_prepared shape : (2000, 13)\n"
     ]
    }
   ],
   "source": [
    "# Import des bibliothèques\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1️⃣ Recharger le dataset\n",
    "df = pd.read_csv(\"../Data/Customer-Churn-Records.csv\")\n",
    "\n",
    "# 2️⃣ Suppression des colonnes inutiles\n",
    "df = df.drop(['RowNumber', 'CustomerId', 'Surname'], axis=1)\n",
    "\n",
    "# 3️⃣ Séparation features / target\n",
    "X = df.drop('Exited', axis=1)\n",
    "y = df['Exited']\n",
    "\n",
    "# 4️⃣ Définition des colonnes numériques et catégorielles\n",
    "cat_cols = ['Geography', 'Card Type', 'Gender']\n",
    "num_cols = ['CreditScore','Age','Tenure','Balance','NumOfProducts','EstimatedSalary','Satisfaction Score','Point Earned']\n",
    "\n",
    "# Encodage binaire pour Gender\n",
    "X['Gender'] = X['Gender'].map({'Female': 0, 'Male': 1})\n",
    "cat_cols.remove('Gender')  # déjà encodée\n",
    "\n",
    "# 5️⃣ Création des transformations\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(drop='first', sparse_output=False)\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, num_cols),\n",
    "        ('cat', categorical_transformer, cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 6️⃣ Création du pipeline complet\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# 7️⃣ Séparation train+val / test (80% / 20%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# 8️⃣ Séparation train / validation (75% train, 25% val)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)\n",
    "\n",
    "# 9️⃣ Application du pipeline\n",
    "X_train_prepared = pipeline.fit_transform(X_train)\n",
    "X_val_prepared = pipeline.transform(X_val)\n",
    "X_test_prepared = pipeline.transform(X_test)\n",
    "\n",
    "\n",
    "# Sauvegarde du pipeline complet\n",
    "joblib.dump(pipeline, 'preprocessing_pipeline.pkl')\n",
    "\n",
    "\n",
    "# Vérification des shapes\n",
    "print(\"X_train_prepared shape :\", X_train_prepared.shape)\n",
    "print(\"X_val_prepared shape :\", X_val_prepared.shape)\n",
    "print(\"X_test_prepared shape :\", X_test_prepared.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad55167-a96d-4dd3-be6b-f4a72d0ad962",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
